"""
ä¼šè©±å±¥æ­´ã‹ã‚‰éèªçŸ¥èƒ½åŠ›ã‚’åˆ†æã™ã‚‹MLãƒ¢ãƒ‡ãƒ«
"""

import os
import re
from dataclasses import dataclass
from typing import Dict, List, Optional

import joblib
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler


@dataclass
class NonCognitiveSkills:
    """éèªçŸ¥èƒ½åŠ›ã‚¹ã‚³ã‚¢"""

    grit: float  # ã‚„ã‚ŠæŠœãåŠ› (0.0-5.0)
    collaboration: float  # å”èª¿æ€§ (0.0-5.0)
    self_regulation: float  # è‡ªå·±åˆ¶å¾¡ (0.0-5.0)
    emotional_intelligence: float  # æ„Ÿæƒ…çŸ¥èƒ½ (0.0-5.0)
    confidence: float = 0.0  # è‡ªä¿¡ (0.0-5.0)


@dataclass
class ConversationMetrics:
    """ä¼šè©±ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    message_count: int
    avg_message_length: float
    question_count: int
    positive_words_count: int
    negative_words_count: int
    learning_keywords_count: int
    help_seeking_count: int
    completion_rate: float  # å•é¡Œè§£æ±ºå®Œäº†ç‡


class ConversationAnalyzer:
    """ä¼šè©±å±¥æ­´ã‹ã‚‰éèªçŸ¥èƒ½åŠ›ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.grit_model = None
        self.collaboration_model = None
        self.self_regulation_model = None
        self.emotional_intelligence_model = None
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words="english")
        self.scaler = StandardScaler()
        self._load_models()

    def _load_models(self):
        """äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        models_dir = os.path.join(os.path.dirname(__file__), "trained_models")

        if os.path.exists(models_dir):
            try:
                self.grit_model = joblib.load(
                    os.path.join(models_dir, "grit_model.pkl")
                )
                self.collaboration_model = joblib.load(
                    os.path.join(models_dir, "collaboration_model.pkl")
                )
                self.self_regulation_model = joblib.load(
                    os.path.join(models_dir, "self_regulation_model.pkl")
                )
                self.emotional_intelligence_model = joblib.load(
                    os.path.join(models_dir, "emotional_intelligence_model.pkl")
                )
                self.vectorizer = joblib.load(
                    os.path.join(models_dir, "vectorizer.pkl")
                )
                self.scaler = joblib.load(os.path.join(models_dir, "scaler.pkl"))
            except FileNotFoundError:
                print(
                    "äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
                )
                self._create_default_models()
        else:
            self._create_default_models()

    def _create_default_models(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        self.grit_model = self._create_rule_based_model()
        self.collaboration_model = self._create_rule_based_model()
        self.self_regulation_model = self._create_rule_based_model()
        self.emotional_intelligence_model = self._create_rule_based_model()

    def _create_rule_based_model(self):
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«"""

        class RuleBasedModel:
            def predict(self, X):
                return np.random.uniform(1.0, 4.0, len(X))

        return RuleBasedModel()

    def analyze_conversation(self, messages: List[Dict]) -> NonCognitiveSkills:
        """ä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã¦éèªçŸ¥èƒ½åŠ›ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º"""

        # ä¼šè©±ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        metrics = self._calculate_conversation_metrics(messages)

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        conversation_text = self._extract_conversation_text(messages)

        # ç‰¹å¾´é‡ã‚’æŠ½å‡º
        features = self._extract_features(conversation_text, metrics)

        # ä¼šè©±å†…å®¹ã«åŸºã¥ãå€‹åˆ¥èª¿æ•´
        content_adjustments = self._analyze_conversation_content(messages)

        # å„éèªçŸ¥èƒ½åŠ›ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ï¼ˆå†…å®¹èª¿æ•´ã‚’é©ç”¨ï¼‰
        grit_score = self._predict_grit(features) + content_adjustments.get("grit", 0)
        collaboration_score = self._predict_collaboration(
            features
        ) + content_adjustments.get("collaboration", 0)
        self_regulation_score = self._predict_self_regulation(
            features
        ) + content_adjustments.get("self_regulation", 0)
        emotional_intelligence_score = self._predict_emotional_intelligence(
            features
        ) + content_adjustments.get("emotional_intelligence", 0)
        confidence_score = self._predict_confidence(features) + content_adjustments.get(
            "confidence", 0
        )

        # ã‚¹ã‚³ã‚¢ã‚’0.0-5.0ã®ç¯„å›²ã«åˆ¶é™
        grit_score = max(0.0, min(5.0, grit_score))
        collaboration_score = max(0.0, min(5.0, collaboration_score))
        self_regulation_score = max(0.0, min(5.0, self_regulation_score))
        emotional_intelligence_score = max(0.0, min(5.0, emotional_intelligence_score))
        confidence_score = max(0.0, min(5.0, confidence_score))

        return NonCognitiveSkills(
            grit=grit_score,
            collaboration=collaboration_score,
            self_regulation=self_regulation_score,
            emotional_intelligence=emotional_intelligence_score,
            confidence=confidence_score,
        )

    def analyze_conversation_with_context(
        self, messages: List[Dict]
    ) -> NonCognitiveSkills:
        """ä¼šè©±å±¥æ­´ã‚’æ–‡è„ˆã‚’å«ã‚ã¦åˆ†æï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹åˆ†æç”¨ï¼‰"""
        if not messages:
            return NonCognitiveSkills()

        # æ™‚ç³»åˆ—ã§ä¼šè©±ã‚’åˆ†æã—ã€æˆé•·ã®è»Œè·¡ã‚’è¿½è·¡
        user_messages = [msg for msg in messages if msg.get("role") == "user"]

        # åŸºæœ¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ
        basic_skills = self.analyze_conversation(messages)

        # æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè£œæ­£ã‚’é©ç”¨
        context_adjustments = self._calculate_context_adjustments(messages)

        # ã‚¹ã‚­ãƒ«ã«æ–‡è„ˆè£œæ­£ã‚’é©ç”¨
        adjusted_skills = NonCognitiveSkills(
            grit=basic_skills.grit + context_adjustments.get("grit", 0),
            collaboration=basic_skills.collaboration
            + context_adjustments.get("collaboration", 0),
            self_regulation=basic_skills.self_regulation
            + context_adjustments.get("self_regulation", 0),
            emotional_intelligence=basic_skills.emotional_intelligence
            + context_adjustments.get("emotional_intelligence", 0),
            confidence=basic_skills.confidence
            + context_adjustments.get("confidence", 0),
        )

        # ã‚¹ã‚³ã‚¢ã‚’0.0-5.0ã®ç¯„å›²ã«åˆ¶é™
        return NonCognitiveSkills(
            grit=max(0.0, min(5.0, adjusted_skills.grit)),
            collaboration=max(0.0, min(5.0, adjusted_skills.collaboration)),
            self_regulation=max(0.0, min(5.0, adjusted_skills.self_regulation)),
            emotional_intelligence=max(
                0.0, min(5.0, adjusted_skills.emotional_intelligence)
            ),
            confidence=max(0.0, min(5.0, adjusted_skills.confidence)),
        )

    def _calculate_context_adjustments(self, messages: List[Dict]) -> Dict[str, float]:
        """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸã‚¹ã‚­ãƒ«è£œæ­£å€¤ã‚’è¨ˆç®—"""
        adjustments = {
            "grit": 0.0,
            "collaboration": 0.0,
            "self_regulation": 0.0,
            "emotional_intelligence": 0.0,
            "confidence": 0.0,
        }

        if len(messages) < 2:
            return adjustments

        # ä¼šè©±ã®ç¶™ç¶šæ€§ã‚’è©•ä¾¡ï¼ˆé•·æœŸé–“ã®ç¶™ç¶šçš„ãªå­¦ç¿’ã‚’ç¤ºã™ï¼‰
        if len(messages) > 10:
            adjustments["grit"] += 0.3  # ç¶™ç¶šæ€§
            adjustments["self_regulation"] += 0.2  # è‡ªå·±ç®¡ç†

        # è³ªå•ã®è³ªã¨æ·±ã•ã‚’è©•ä¾¡
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        question_quality = self._evaluate_question_quality(user_messages)

        if question_quality > 0.7:
            adjustments["confidence"] += 0.2  # è‡ªä¿¡
            adjustments["emotional_intelligence"] += 0.1  # æ„Ÿæƒ…ç†è§£

        # å­¦ç¿’ãƒˆãƒ”ãƒƒã‚¯ã®å¤šæ§˜æ€§ã‚’è©•ä¾¡
        topic_diversity = self._evaluate_topic_diversity(user_messages)
        if topic_diversity > 0.6:
            adjustments["collaboration"] += 0.2  # å”èª¿æ€§
            adjustments["emotional_intelligence"] += 0.1  # æ„Ÿæƒ…ç†è§£

        return adjustments

    def _evaluate_question_quality(self, user_messages: List[Dict]) -> float:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®è³ªã‚’è©•ä¾¡"""
        if not user_messages:
            return 0.0

        quality_indicators = [
            "ãªãœ",
            "ã©ã®ã‚ˆã†ã«",
            "ãªãœãªã‚‰",
            "ä¾‹ãˆã°",
            "å…·ä½“çš„ã«",
            "é•ã„ã¯",
            "é–¢ä¿‚ã¯",
            "æ„å‘³ã¯",
            "ç†ç”±ã¯",
            "æ–¹æ³•ã¯",
        ]

        total_questions = len(user_messages)
        quality_questions = 0

        for msg in user_messages:
            content = msg.get("content", "").lower()
            if any(indicator in content for indicator in quality_indicators):
                quality_questions += 1

        return quality_questions / total_questions if total_questions > 0 else 0.0

    def _evaluate_topic_diversity(self, user_messages: List[Dict]) -> float:
        """å­¦ç¿’ãƒˆãƒ”ãƒƒã‚¯ã®å¤šæ§˜æ€§ã‚’è©•ä¾¡"""
        if not user_messages:
            return 0.0

        topics = set()
        topic_keywords = {
            "æ•°å­¦": ["æ•°å­¦", "ç®—æ•°", "è¨ˆç®—", "æ–¹ç¨‹å¼", "å¹¾ä½•", "ä»£æ•°"],
            "å›½èª": ["å›½èª", "æ—¥æœ¬èª", "æ–‡ç« ", "èª­è§£", "ä½œæ–‡", "æ–‡æ³•"],
            "ç†ç§‘": ["ç†ç§‘", "ç§‘å­¦", "å®Ÿé¨“", "ç‰©ç†", "åŒ–å­¦", "ç”Ÿç‰©"],
            "ç¤¾ä¼š": ["ç¤¾ä¼š", "æ­´å²", "åœ°ç†", "æ”¿æ²»", "çµŒæ¸ˆ", "æ–‡åŒ–"],
            "è‹±èª": ["è‹±èª", "è‹±ä¼šè©±", "å˜èª", "æ–‡æ³•", "ãƒªã‚¹ãƒ‹ãƒ³ã‚°"],
            "èŠ¸è¡“": ["èŠ¸è¡“", "éŸ³æ¥½", "ç¾è¡“", "çµµç”»", "æ¼”å¥"],
            "ä½“è‚²": ["ä½“è‚²", "é‹å‹•", "ã‚¹ãƒãƒ¼ãƒ„", "å¥åº·", "ä½“åŠ›"],
        }

        for msg in user_messages:
            content = msg.get("content", "").lower()
            for topic, keywords in topic_keywords.items():
                if any(keyword in content for keyword in keywords):
                    topics.add(topic)

        return len(topics) / len(topic_keywords)

    def _calculate_conversation_metrics(
        self, messages: List[Dict]
    ) -> ConversationMetrics:
        """ä¼šè©±ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        user_messages = [msg for msg in messages if msg.get("role") == "user"]

        total_messages = len(user_messages)
        total_length = sum(len(msg.get("content", "")) for msg in user_messages)
        avg_length = total_length / max(total_messages, 1)

        # è³ªå•ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        question_count = sum(
            1
            for msg in user_messages
            if "?" in msg.get("content", "")
            or "ã§ã™ã‹" in msg.get("content", "")
            or "ã§ã—ã‚‡ã†ã‹" in msg.get("content", "")
        )

        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        positive_words = [
            "é ‘å¼µã‚‹",
            "åŠªåŠ›",
            "ç¶šã‘ã‚‹",
            "æŒ‘æˆ¦",
            "å­¦ã¶",
            "ç†è§£",
            "ã§ããŸ",
            "æˆåŠŸ",
            "å¬‰ã—ã„",
            "æ¥½ã—ã„",
        ]
        negative_words = [
            "é›£ã—ã„",
            "ã‚ã‹ã‚‰ãªã„",
            "ã§ããªã„",
            "å›°ã£ãŸ",
            "ç–²ã‚ŒãŸ",
            "å«Œã„",
            "è‹¦æ‰‹",
            "å¤±æ•—",
        ]

        all_text = " ".join(msg.get("content", "") for msg in user_messages)
        positive_count = sum(all_text.count(word) for word in positive_words)
        negative_count = sum(all_text.count(word) for word in negative_words)

        # å­¦ç¿’é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        learning_keywords = [
            "å‹‰å¼·",
            "å­¦ç¿’",
            "å•é¡Œ",
            "èª²é¡Œ",
            "å®¿é¡Œ",
            "æˆæ¥­",
            "æ•™ç§‘æ›¸",
            "ãƒ†ã‚¹ãƒˆ",
            "è©¦é¨“",
        ]
        learning_keywords_count = sum(
            all_text.count(word) for word in learning_keywords
        )

        # ãƒ˜ãƒ«ãƒ—ã‚·ãƒ¼ã‚­ãƒ³ã‚°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        help_words = [
            "æ•™ãˆã¦",
            "åŠ©ã‘ã¦",
            "æ‰‹ä¼ã£ã¦",
            "ã‚ã‹ã‚‰ãªã„",
            "å›°ã£ã¦ã„ã‚‹",
            "ã©ã†ã™ã‚Œã°",
        ]
        help_seeking_count = sum(all_text.count(word) for word in help_words)

        # å®Œäº†ç‡ï¼ˆç°¡æ˜“ç‰ˆï¼šè³ªå•ã«å¯¾ã™ã‚‹è¿”ç­”ã®æœ‰ç„¡ï¼‰
        completion_rate = min(
            1.0, total_messages / max(1, len(messages) - total_messages)
        )

        return ConversationMetrics(
            message_count=total_messages,
            avg_message_length=avg_length,
            question_count=question_count,
            positive_words_count=positive_count,
            negative_words_count=negative_count,
            learning_keywords_count=learning_keywords_count,
            help_seeking_count=help_seeking_count,
            completion_rate=completion_rate,
        )

    def _extract_conversation_text(self, messages: List[Dict]) -> str:
        """ä¼šè©±ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        return " ".join(msg.get("content", "") for msg in user_messages)

    def _extract_features(
        self, conversation_text: str, metrics: ConversationMetrics
    ) -> np.ndarray:
        """ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        features = [
            metrics.message_count,
            metrics.avg_message_length,
            metrics.question_count,
            metrics.positive_words_count,
            metrics.negative_words_count,
            metrics.learning_keywords_count,
            metrics.help_seeking_count,
            metrics.completion_rate,
            len(conversation_text.split()),
            len(re.findall(r"[!ï¼]", conversation_text)),  # æ„Ÿå˜†ç¬¦ã®æ•°
            len(re.findall(r"[?ï¼Ÿ]", conversation_text)),  # ç–‘å•ç¬¦ã®æ•°
        ]

        return np.array(features).reshape(1, -1)

    def _predict_grit(self, features: np.ndarray) -> float:
        """ã‚°ãƒªãƒƒãƒˆï¼ˆã‚„ã‚ŠæŠœãåŠ›ï¼‰ã‚’äºˆæ¸¬"""
        if self.grit_model:
            score = self.grit_model.predict(features)[0]
        else:
            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“äºˆæ¸¬
            score = min(
                5.0, max(1.0, 2.0 + features[0][2] * 0.1 + features[0][3] * 0.2)
            )
        return round(score, 2)

    def _predict_collaboration(self, features: np.ndarray) -> float:
        """å”èª¿æ€§ã‚’äºˆæ¸¬"""
        if self.collaboration_model:
            score = self.collaboration_model.predict(features)[0]
        else:
            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“äºˆæ¸¬
            score = min(
                5.0, max(1.0, 2.0 + features[0][6] * 0.3 + features[0][9] * 0.1)
            )
        return round(score, 2)

    def _predict_self_regulation(self, features: np.ndarray) -> float:
        """è‡ªå·±åˆ¶å¾¡ã‚’äºˆæ¸¬"""
        if self.self_regulation_model:
            score = self.self_regulation_model.predict(features)[0]
        else:
            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“äºˆæ¸¬
            score = min(
                5.0, max(1.0, 2.0 + features[0][1] * 0.1 + features[0][7] * 0.4)
            )
        return round(score, 2)

    def _predict_emotional_intelligence(self, features: np.ndarray) -> float:
        """æ„Ÿæƒ…çŸ¥èƒ½ã‚’äºˆæ¸¬"""
        if self.emotional_intelligence_model:
            score = self.emotional_intelligence_model.predict(features)[0]
        else:
            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“äºˆæ¸¬
            score = min(
                5.0, max(1.0, 2.0 + features[0][3] * 0.2 - features[0][4] * 0.1)
            )
        return round(score, 2)

    def _predict_confidence(self, features: np.ndarray) -> float:
        """è‡ªä¿¡ã‚’äºˆæ¸¬"""
        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“äºˆæ¸¬
        score = min(5.0, max(1.0, 2.0 + features[0][3] * 0.2 + features[0][7] * 0.3))
        return round(score, 2)

    def generate_feedback(
        self,
        skills: NonCognitiveSkills,
        previous_skills: Optional[NonCognitiveSkills] = None,
    ) -> str:
        """éèªçŸ¥èƒ½åŠ›ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦å€‹åˆ¥åŒ–ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆ"""

        import random

        feedback_parts = []

        # ã‚ˆã‚Šå¤šæ§˜ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        grit_feedbacks = {
            "high": [
                "ğŸŒŸ ç´ æ™´ã‚‰ã—ã„ã‚„ã‚ŠæŠœãåŠ›ï¼å›°é›£ãªèª²é¡Œã«ã‚‚è«¦ã‚ãšã«å–ã‚Šçµ„ã‚€å§¿å‹¢ãŒå°è±¡çš„ã§ã™ã€‚",
                "ğŸ’ª ç¶™ç¶šåŠ›ãŒæŠœç¾¤ã§ã™ã­ï¼ç›®æ¨™ã«å‘ã‹ã£ã¦ä¸€æ­©ä¸€æ­©é€²ã‚€å§¿å‹¢ãŒç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚",
                "ğŸ† ç²˜ã‚Šå¼·ã•ãŒéš›ç«‹ã£ã¦ã„ã¾ã™ï¼ã©ã‚“ãªå›°é›£ã‚‚ä¹—ã‚Šè¶Šãˆã‚‰ã‚Œã‚‹åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã­ã€‚",
                "ğŸ¯ ç›®æ¨™é”æˆã¸ã®æ„å¿—ãŒå¼·ãæ„Ÿã˜ã‚‰ã‚Œã¾ã™ï¼ã“ã®èª¿å­ã§é ‘å¼µã£ã¦ãã ã•ã„ã€‚",
            ],
            "medium": [
                "ğŸ‘ ã‚„ã‚ŠæŠœãåŠ›ãŒè‚²ã£ã¦ã„ã¾ã™ï¼ã‚‚ã†å°‘ã—ç¶™ç¶šã™ã‚‹ã“ã¨ã§å¤§ããªæˆæœãŒå¾—ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚",
                "ğŸ“ˆ åŠªåŠ›ã™ã‚‹å§¿å‹¢ãŒè¦‹ãˆã¦ãã¾ã—ãŸï¼å°ã•ãªæˆåŠŸã‚’ç©ã¿é‡ã­ã¦ã„ãã¾ã—ã‚‡ã†ã€‚",
                "ğŸ”„ ç¶™ç¶šæ€§ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ï¼ç¿’æ…£åŒ–ã™ã‚‹ã“ã¨ã§ã•ã‚‰ã«åŠ›ã‚’ä¼¸ã°ã›ã¾ã™ã€‚",
                "âš¡ é›†ä¸­åŠ›ãŒé«˜ã¾ã£ã¦ãã¦ã„ã¾ã™ï¼ã“ã®å‹¢ã„ã§å­¦ç¿’ã‚’ç¶šã‘ã¦ã¿ã¦ãã ã•ã„ã€‚",
            ],
            "low": [
                "ğŸ’¡ ã‚„ã‚ŠæŠœãåŠ›ã‚’è‚²ã¦ã‚‹ãŸã‚ã«ã€ã¾ãšã¯å°ã•ãªç›®æ¨™ã‹ã‚‰å§‹ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸª æ¥½ã—ãç¶™ç¶šã§ãã‚‹æ–¹æ³•ã‚’è¦‹ã¤ã‘ã¦ã€å­¦ç¿’ã‚’ç¿’æ…£ã«ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚",
                "ğŸŒ± æˆé•·ã®ç¨®ã‚’ã¾ã„ã¦ã„ã‚‹æ®µéšã§ã™ï¼ç„¦ã‚‰ãšã«ä¸€ã¤ãšã¤å–ã‚Šçµ„ã‚“ã§ã¿ã¦ãã ã•ã„ã€‚",
                "ğŸš€ ã‚¹ã‚¿ãƒ¼ãƒˆãƒ©ã‚¤ãƒ³ã«ç«‹ã£ã¦ã„ã¾ã™ï¼å°ã•ãªä¸€æ­©ã‹ã‚‰å§‹ã‚ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ",
            ],
        }

        collaboration_feedbacks = {
            "high": [
                "ğŸ¤ å”èª¿æ€§ãŒç´ æ™´ã‚‰ã—ã„ã§ã™ï¼ä»–è€…ã¨ã®å”åŠ›ã‚’å¤§åˆ‡ã«ã™ã‚‹å§¿å‹¢ãŒå°è±¡çš„ã§ã™ã€‚",
                "ğŸ‘¥ ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ‰èƒ½ãŒã‚ã‚Šã¾ã™ï¼ã¿ã‚“ãªã¨ä¸€ç·’ã«å­¦ã¶æ¥½ã—ã•ã‚’æ„Ÿã˜ã¦ã„ã¾ã™ã­ã€‚",
                "ğŸ¤² æ€ã„ã‚„ã‚Šã®ã‚ã‚‹å­¦ç¿’å§¿å‹¢ãŒç´ æ™´ã‚‰ã—ã„ã§ã™ï¼å‘¨ã‚Šã®äººã‚‚æ”¯ãˆã¦ã„ã‚‹ã§ã—ã‚‡ã†ã€‚",
                "ğŸ­ ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›ãŒé«˜ã„ã§ã™ã­ï¼ç©æ¥µçš„ã«äº¤æµã™ã‚‹å§¿å‹¢ãŒè‰¯ã„ã§ã™ã€‚",
            ],
            "medium": [
                "ğŸ‘‚ å”èª¿æ€§ãŒè‚²ã£ã¦ã„ã¾ã™ï¼ç›¸æ‰‹ã®æ„è¦‹ã«è€³ã‚’å‚¾ã‘ã‚‹å§¿å‹¢ãŒè¦‹ãˆã¦ãã¾ã—ãŸã€‚",
                "ğŸ’¬ ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ï¼è³ªå•ã‚„ç›¸è«‡ã‚’ç©æ¥µçš„ã«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸ¤ å”åŠ›ã™ã‚‹å§¿å‹¢ãŒè¦‹ãˆã¦ãã¾ã—ãŸï¼ã‚°ãƒ«ãƒ¼ãƒ—å­¦ç¿’ã§ã•ã‚‰ã«åŠ›ã‚’ä¼¸ã°ã›ã¾ã™ã€‚",
                "ğŸ‘¥ ç¤¾äº¤æ€§ãŒé«˜ã¾ã£ã¦ã„ã¾ã™ï¼å‹é”ã¨ä¸€ç·’ã«å­¦ç¿’ã™ã‚‹æ©Ÿä¼šã‚’å¢—ã‚„ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
            ],
            "low": [
                "ğŸ—£ï¸ å”èª¿æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€ã¾ãšã¯è³ªå•ã‚„ç›¸è«‡ã‚’ç©æ¥µçš„ã«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸ‘¥ ã‚°ãƒ«ãƒ¼ãƒ—å­¦ç¿’ã«å‚åŠ ã—ã¦ã€ä»–ã®äººã¨ã®äº¤æµã‚’æ¥½ã—ã‚“ã§ã¿ã¦ãã ã•ã„ã€‚",
                "ğŸ’­ è‡ªåˆ†ã®æ„è¦‹ã‚’ä¼ãˆã‚‹ç·´ç¿’ã‹ã‚‰å§‹ã‚ã¦ã€å”èª¿æ€§ã‚’è‚²ã¦ã¦ã„ãã¾ã—ã‚‡ã†ã€‚",
                "ğŸ¤² ä»–è€…ã¸ã®æ€ã„ã‚„ã‚Šã‚’æ„è­˜ã—ã¦ã€å”åŠ›çš„ãªå§¿å‹¢ã‚’èº«ã«ã¤ã‘ã¦ã„ãã¾ã—ã‚‡ã†ã€‚",
            ],
        }

        self_regulation_feedbacks = {
            "high": [
                "ğŸ¯ è‡ªå·±ç®¡ç†èƒ½åŠ›ãŒå„ªã‚Œã¦ã„ã¾ã™ï¼è¨ˆç”»çš„ã«å­¦ç¿’ã‚’é€²ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã­ã€‚",
                "â° æ™‚é–“ç®¡ç†ãŒç´ æ™´ã‚‰ã—ã„ã§ã™ï¼åŠ¹ç‡çš„ãªå­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ãŒèº«ã«ã¤ã„ã¦ã„ã¾ã™ã€‚",
                "ğŸ“‹ è¨ˆç”»æ€§ãŒæŠœç¾¤ã§ã™ã­ï¼ç›®æ¨™ã«å‘ã‹ã£ã¦ç€å®Ÿã«é€²ã‚“ã§ã„ã¾ã™ã€‚",
                "ğŸ§  é›†ä¸­åŠ›ã¨è‡ªåˆ¶å¿ƒãŒé«˜ã„ã§ã™ï¼å­¦ç¿’ã«å–ã‚Šçµ„ã‚€å§¿å‹¢ãŒç´ æ™´ã‚‰ã—ã„ã§ã™ã€‚",
            ],
            "medium": [
                "ğŸ“ è‡ªå·±ç®¡ç†åŠ›ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ï¼å­¦ç¿’è¨ˆç”»ã‚’ç«‹ã¦ã¦å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                "â±ï¸ æ™‚é–“ã®ä½¿ã„æ–¹ãŒæ”¹å–„ã•ã‚Œã¦ãã¾ã—ãŸï¼ã•ã‚‰ã«åŠ¹ç‡åŒ–ã‚’å›³ã£ã¦ã¿ã¦ãã ã•ã„ã€‚",
                "ğŸ“Š è¨ˆç”»æ€§ãŒè‚²ã£ã¦ã„ã¾ã™ï¼ç›®æ¨™è¨­å®šã‚’æ˜ç¢ºã«ã—ã¦å–ã‚Šçµ„ã‚“ã§ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸª é›†ä¸­åŠ›ãŒé«˜ã¾ã£ã¦ãã¦ã„ã¾ã™ï¼å­¦ç¿’ç’°å¢ƒã‚’æ•´ãˆã¦ã•ã‚‰ã«åŠ›ã‚’ä¼¸ã°ã—ã¾ã—ã‚‡ã†ã€‚",
            ],
            "low": [
                "ğŸ“… è‡ªå·±ç®¡ç†åŠ›ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€ã¾ãšã¯å­¦ç¿’æ™‚é–“ã‚’æ±ºã‚ã¦å–ã‚Šçµ„ã‚“ã§ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸ¯ ç›®æ¨™è¨­å®šã‹ã‚‰å§‹ã‚ã¦ã€è¨ˆç”»çš„ã«å­¦ç¿’ã‚’é€²ã‚ã¦ã„ãã¾ã—ã‚‡ã†ã€‚",
                "â° æ™‚é–“ç®¡ç†ã®ç·´ç¿’ã‚’ã—ã¦ã€åŠ¹ç‡çš„ãªå­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã‚’èº«ã«ã¤ã‘ã¾ã—ã‚‡ã†ã€‚",
                "ğŸ§˜ é›†ä¸­åŠ›ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€å­¦ç¿’ç’°å¢ƒã‚’æ•´ãˆã¦å–ã‚Šçµ„ã‚“ã§ã¿ã¾ã—ã‚‡ã†ã€‚",
            ],
        }

        emotional_intelligence_feedbacks = {
            "high": [
                "ğŸ’ æ„Ÿæƒ…çŸ¥èƒ½ãŒé«˜ã„ã§ã™ï¼è‡ªåˆ†ã®æ„Ÿæƒ…ã‚’ç†è§£ã—ã€é©åˆ‡ã«è¡¨ç¾ã§ãã¦ã„ã¾ã™ã€‚",
                "ğŸ˜Š æ„Ÿæƒ…ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãŒç´ æ™´ã‚‰ã—ã„ã§ã™ï¼å®‰å®šã—ãŸå­¦ç¿’å§¿å‹¢ãŒå°è±¡çš„ã§ã™ã€‚",
                "ğŸŒˆ æ„Ÿæƒ…ã®è±Šã‹ã•ã¨è¡¨ç¾åŠ›ãŒå„ªã‚Œã¦ã„ã¾ã™ï¼å­¦ç¿’ã«ã‚‚è‰¯ã„å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™ã­ã€‚",
                "ğŸ¤— å…±æ„ŸåŠ›ãŒé«˜ãã€ä»–è€…ã¨ã®é–¢ä¿‚æ€§ã‚‚è‰¯å¥½ã§ã™ã­ï¼å­¦ç¿’ç’°å¢ƒã‚‚è‰¯ããªã£ã¦ã„ã‚‹ã§ã—ã‚‡ã†ã€‚",
            ],
            "medium": [
                "ğŸ˜Œ æ„Ÿæƒ…çŸ¥èƒ½ãŒè‚²ã£ã¦ã„ã¾ã™ï¼æ„Ÿæƒ…ã‚’è¨€è‘‰ã§è¡¨ç¾ã™ã‚‹ç·´ç¿’ã‚’ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸ§˜ æ„Ÿæƒ…ã®å®‰å®šæ€§ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ï¼ã‚¹ãƒˆãƒ¬ã‚¹ç®¡ç†ã‚‚æ„è­˜ã—ã¦ã¿ã¦ãã ã•ã„ã€‚",
                "ğŸ’­ è‡ªå·±ç†è§£ãŒæ·±ã¾ã£ã¦ãã¾ã—ãŸï¼æ„Ÿæƒ…ã‚’æŒ¯ã‚Šè¿”ã‚‹æ™‚é–“ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸ­ æ„Ÿæƒ…è¡¨ç¾ãŒè±Šã‹ã«ãªã£ã¦ãã¾ã—ãŸï¼å­¦ç¿’ã¸ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚‚é«˜ã¾ã£ã¦ã„ã‚‹ã§ã—ã‚‡ã†ã€‚",
            ],
            "low": [
                "ğŸ’­ æ„Ÿæƒ…çŸ¥èƒ½ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€è‡ªåˆ†ã®æ°—æŒã¡ã‚’æŒ¯ã‚Šè¿”ã‚‹æ™‚é–“ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
                "ğŸ˜Š ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…ã‚’æ„è­˜ã—ã¦ã€å­¦ç¿’ã¸ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é«˜ã‚ã¦ã„ãã¾ã—ã‚‡ã†ã€‚",
                "ğŸ§  æ„Ÿæƒ…ã¨æ€è€ƒã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã£ã¦ã€å®‰å®šã—ãŸå­¦ç¿’å§¿å‹¢ã‚’èº«ã«ã¤ã‘ã¾ã—ã‚‡ã†ã€‚",
                "ğŸŒˆ æ„Ÿæƒ…ã®è¡¨ç¾åŠ›ã‚’é«˜ã‚ã¦ã€å­¦ç¿’ã¸ã®æ„æ¬²ã‚’è‚²ã¦ã¦ã„ãã¾ã—ã‚‡ã†ã€‚",
            ],
        }

        # ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é¸æŠ
        def get_feedback_level(score):
            if score >= 4.0:
                return "high"
            elif score >= 3.0:
                return "medium"
            else:
                return "low"

        # å„ã‚¹ã‚­ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆï¼ˆãƒ©ãƒ³ãƒ€ãƒ é¸æŠï¼‰
        grit_level = get_feedback_level(skills.grit)
        feedback_parts.append(random.choice(grit_feedbacks[grit_level]))

        collaboration_level = get_feedback_level(skills.collaboration)
        feedback_parts.append(
            random.choice(collaboration_feedbacks[collaboration_level])
        )

        self_regulation_level = get_feedback_level(skills.self_regulation)
        feedback_parts.append(
            random.choice(self_regulation_feedbacks[self_regulation_level])
        )

        emotional_intelligence_level = get_feedback_level(skills.emotional_intelligence)
        feedback_parts.append(
            random.choice(
                emotional_intelligence_feedbacks[emotional_intelligence_level]
            )
        )

        # é€²æ­©ã®è©•ä¾¡
        if previous_skills:
            improvements = []
            if skills.grit > previous_skills.grit + 0.2:
                improvements.append("ã‚„ã‚ŠæŠœãåŠ›")
            if skills.collaboration > previous_skills.collaboration + 0.2:
                improvements.append("å”èª¿æ€§")
            if skills.self_regulation > previous_skills.self_regulation + 0.2:
                improvements.append("è‡ªå·±åˆ¶å¾¡åŠ›")
            if (
                skills.emotional_intelligence
                > previous_skills.emotional_intelligence + 0.2
            ):
                improvements.append("æ„Ÿæƒ…çŸ¥èƒ½")

            if improvements:
                feedback_parts.append(
                    f"ğŸ‰ ç´ æ™´ã‚‰ã—ã„é€²æ­©ã§ã™ï¼{', '.join(improvements)}ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚"
                )

        return "\n\n".join(feedback_parts)

    def _analyze_conversation_content(self, messages: List[Dict]) -> Dict[str, float]:
        """ä¼šè©±å†…å®¹ã«åŸºã¥ã„ã¦ã‚¹ã‚­ãƒ«èª¿æ•´å€¤ã‚’è¨ˆç®—"""

        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        all_text = " ".join(msg.get("content", "") for msg in user_messages).lower()

        adjustments = {
            "grit": 0.0,
            "collaboration": 0.0,
            "self_regulation": 0.0,
            "emotional_intelligence": 0.0,
            "confidence": 0.0,
        }

        # ã‚°ãƒªãƒƒãƒˆé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        grit_positive = [
            "é ‘å¼µã‚‹",
            "ç¶šã‘ã‚‹",
            "æŒ‘æˆ¦",
            "åŠªåŠ›",
            "ç›®æ¨™",
            "é”æˆ",
            "ã‚„ã‚ŠæŠœã",
            "è«¦ã‚ãªã„",
        ]
        grit_negative = ["è«¦ã‚ã‚‹", "ã‚„ã‚ã‚‹", "é¢å€’", "ç–²ã‚ŒãŸ", "ç„¡ç†"]

        grit_score = sum(1 for word in grit_positive if word in all_text) * 0.1
        grit_score -= sum(1 for word in grit_negative if word in all_text) * 0.05
        adjustments["grit"] = grit_score

        # å”èª¿æ€§é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        collaboration_positive = [
            "ä¸€ç·’",
            "å”åŠ›",
            "åŠ©ã‘ã‚‹",
            "è³ªå•",
            "ç›¸è«‡",
            "æ•™ãˆã‚‹",
            "ã‚°ãƒ«ãƒ¼ãƒ—",
            "ãƒãƒ¼ãƒ ",
        ]
        collaboration_negative = ["ä¸€äºº", "ç‹¬ã‚Š", "è‡ªåˆ†ã ã‘", "ä»–äºº", "é‚ªé­”"]

        collab_score = (
            sum(1 for word in collaboration_positive if word in all_text) * 0.1
        )
        collab_score -= (
            sum(1 for word in collaboration_negative if word in all_text) * 0.05
        )
        adjustments["collaboration"] = collab_score

        # è‡ªå·±åˆ¶å¾¡é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        regulation_positive = ["è¨ˆç”»", "æ™‚é–“", "é›†ä¸­", "è¨ˆç”»çš„", "æ•´ç†", "ç®¡ç†", "ç¿’æ…£"]
        regulation_negative = ["ã ã‚‰ã ã‚‰", "æ•£æ¼«", "é›†ä¸­ã§ããªã„", "è¨ˆç”»ãªã—"]

        reg_score = sum(1 for word in regulation_positive if word in all_text) * 0.1
        reg_score -= sum(1 for word in regulation_negative if word in all_text) * 0.05
        adjustments["self_regulation"] = reg_score

        # æ„Ÿæƒ…çŸ¥èƒ½é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        emotion_positive = [
            "å¬‰ã—ã„",
            "æ¥½ã—ã„",
            "æ„Ÿæƒ…",
            "æ°—æŒã¡",
            "ç†è§£",
            "å…±æ„Ÿ",
            "æ„Ÿè¬",
        ]
        emotion_negative = ["æ€’ã‚‹", "ã‚¤ãƒ©ã‚¤ãƒ©", "æ‚²ã—ã„", "ä¸å®‰", "ã‚¹ãƒˆãƒ¬ã‚¹"]

        emotion_score = sum(1 for word in emotion_positive if word in all_text) * 0.1
        emotion_score -= sum(1 for word in emotion_negative if word in all_text) * 0.03
        adjustments["emotional_intelligence"] = emotion_score

        # è‡ªä¿¡é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        confidence_positive = ["ã§ãã‚‹", "å¤§ä¸ˆå¤«", "è‡ªä¿¡", "æˆåŠŸ", "å¾—æ„", "å¥½ã"]
        confidence_negative = ["ã§ããªã„", "ç„¡ç†", "è‡ªä¿¡ãªã„", "è‹¦æ‰‹", "å«Œã„"]

        conf_score = sum(1 for word in confidence_positive if word in all_text) * 0.1
        conf_score -= sum(1 for word in confidence_negative if word in all_text) * 0.05
        adjustments["confidence"] = conf_score

        # èª¿æ•´å€¤ã‚’-0.5ã‹ã‚‰+0.5ã®ç¯„å›²ã«åˆ¶é™
        for key in adjustments:
            adjustments[key] = max(-0.5, min(0.5, adjustments[key]))

        return adjustments
